{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston House pricing Binary Classification problem\n",
    "\n",
    "Case: Return YES if the new house is predicted to be worth more than $22000. No if not.  \n",
    "\n",
    "1. Load dataset onto notebook instance from S3\n",
    "2. Clean, transform and Prepare the dataset\n",
    "3. Create and train linear learner model\n",
    "4. Deploy the model into SageMaker hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Load the data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "bucket = 'boston-house-bucket'\n",
    "sub_folder = 'boston-house-data'\n",
    "data_key = 'boston_housing_raw.csv'\n",
    "data_location = 's3://{}/{}/{}'.format(bucket, sub_folder, data_key)\n",
    "\n",
    "df = pd.read_csv(data_location, low_memory = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Clean, Transform and Prepare the dataset\n",
    "\n",
    "\n",
    "See [Variable description](http://lib.stat.cmu.edu/datasets/boston)\n",
    "\n",
    "1. Convert CHAS, RAD varibales into categorical and one-hot encode them\n",
    "2. MinMaxScale the data so that all the points will be in 0 to 1 range\n",
    "3. Find the scaled value for $22000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are any missing values\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unrequired columns\n",
    "df.drop(columns = ['Unnamed: 0'], inplace = True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert CHAS, RAD attributes to categorical\n",
    "df['CHAS'] = df['CHAS'].astype('category')\n",
    "df['RAD'] = df['RAD'].astype('category')\n",
    "\n",
    "#one-hot encode CHAS, RAD attributes\n",
    "df = pd.get_dummies(df, columns=['CHAS', 'RAD'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data to evenly distribute between 0 and 1\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minmaxscaler converts dataframe to ndarray, convert it back to data frame\n",
    "df_scaled = pd.DataFrame(data = data_scaled, columns = list(df) )\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled['MEDV'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled['MEDV'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MEDV'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this calculation gives scaled down value for any single number(i) if its in MEDV range.\n",
    "x = df['MEDV']\n",
    "i = 22\n",
    "\n",
    "if i in range(len(x)):\n",
    "    i_scl = ([(i - min(x)) / (max(x) - min(x))]) \n",
    "    print(\"Scaled value of i:\", i_scl)\n",
    "else:\n",
    "    print('Value not in range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create and Train Linear Learner  model\n",
    "\n",
    "1. Randomize data\n",
    "2. Split data into train, validate and test sets\n",
    "3. Classify label MEDV data points to 1(yes) if above USD20000(scaled value 0.377), 0(no) if below.\n",
    "4. Convert data sets into recordIO format and upload into S3\n",
    "\n",
    "#### Training Job\n",
    "1. Import the Amazon SageMaker Python SDK and get the linear-learner container\n",
    "2. Create training job name(must be unique for every run) and output location\n",
    "3. Set up required parameters for linear learner algorithm. See [details](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model.html).\n",
    "4. Set up hyperparameters. Use SageMaker hyperparameter tuning jobs for optimized values.\n",
    "5. Pass the training, validation channels for input\n",
    "6. To start model training, call the estimator's fit method. This method calls the CreateTrainingJob API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomize data and split data into train, validation and test sets\n",
    "np.random.seed(0)\n",
    "\n",
    "rand_split = np.random.rand(len(df_scaled))\n",
    "\n",
    "train_list = rand_split < 0.8\n",
    "val_list = (rand_split >= 0.8) & (rand_split < 0.9)\n",
    "test_list = rand_split <= 0.9\n",
    "\n",
    "#datasets for training, validating and testing\n",
    "data_train = df_scaled[train_list]\n",
    "data_val = df_scaled[val_list]\n",
    "data_test = df_scaled[test_list]\n",
    "\n",
    "#convert data sets into numpy.ndarray. X is features and Y is labels\n",
    "\n",
    "train_X = data_train.drop(columns = 'MEDV').to_numpy() \n",
    "train_Y = ((data_train['MEDV'] > 0.377777)+0).to_numpy() #values above 0.37 will return as 1, and below will be as 0.\n",
    "\n",
    "val_X = data_val.drop(columns = 'MEDV').to_numpy()\n",
    "val_Y = ((data_val['MEDV'] > 0.377777)+0).to_numpy()\n",
    "\n",
    "test_X = data_test.drop(columns = 'MEDV').to_numpy()\n",
    "test_Y = ((data_val['MEDV'] > 0.377777)+0).to_numpy()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create recordIO protobuf type float32 for training data\n",
    "train_file = 'boston_housing_train_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, train_X.astype('float32'),\n",
    "                                train_Y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "#Upload to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('linearlearner_train/{}'.format(train_file)).upload_fileobj(f)\n",
    "\n",
    "#location of the training data in S3\n",
    "train_channel = 's3://{}/linearlearner_train/{}'.format(bucket,train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create recordIO protobuf type32 for validation data\n",
    "validation_file = 'boston_housing_validation_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, val_X.astype('float32'),\n",
    "                                val_Y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "#upload to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('linearlearner_validation/{}'.format(validation_file,)).upload_fileobj(f)\n",
    "\n",
    "#location of the validation data in S3\n",
    "validation_channel = 's3://{}/linearlearner_validation/{}'.format(bucket,validation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Amazon SageMaker Python SDK and get the linear-learner container.\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner',\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a training job name\n",
    "job_name = 'bh-linear-learner-job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print('job name{}'.format(job_name))\n",
    "\n",
    "#output path of the model artifacts\n",
    "output_location = 's3://{}/linearlearner-output'.format(bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The feature_dim hyperparameter needs to be set to {}.'.format(data_train.shape[1]-1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session objest manages interactions with necassary AWS services\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "#set up linear algorithm from ECR\n",
    "linear = sagemaker.estimator.Estimator(container,\n",
    "                                      role,\n",
    "                                      train_instance_count =1,\n",
    "                                      train_instance_type = 'ml.c4.xlarge',\n",
    "                                      output_path=output_location,\n",
    "                                      sagemaker_session=sess,\n",
    "                                      input_mode='Pipe')\n",
    "\n",
    "#set up hyperparameters.\n",
    "linear.set_hyperparameters(feature_dim = 22,\n",
    "                          predictor_type = 'binary_classifier',\n",
    "                          l1 = 0.0034313572059783636,\n",
    "                          learning_rate = 0.022529489694206588,\n",
    "                          mini_batch_size = 1,\n",
    "                          use_bias = 'true',\n",
    "                          wd = 0.08134206001008425)\n",
    "\n",
    "#launch training job. This method calls the CreateTrainingJob API call\n",
    "data_channels = {\n",
    "    'train': train_channel,\n",
    "    'validation': validation_channel\n",
    "}\n",
    "linear.fit(data_channels, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('location of the model:{}/{}/model.tar.gz'.format(output_location, job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Deploy the model into SageMaker hosting\n",
    "\n",
    "call .deploy method to deploy the model into SageMaker Hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryclass_predictor = linear.deploy(initial_instance_count =1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After deploying the model <br>\n",
    "1. Set up the confusion matrix\n",
    "2. Run the batch predictions on test data\n",
    "3. Run confusion matrix\n",
    "4. Print the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None, \n",
    "                          cmap=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "            plt.cm.Greens\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='Actual',\n",
    "           xlabel='Predicted')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_deserializer, csv_serializer\n",
    "\n",
    "binaryclass_predictor.content_type = 'text/csv'\n",
    "binaryclass_predictor.serializer = csv_serializer\n",
    "binaryclass_predictor.deserializer = json_deserializer\n",
    "\n",
    "predictions = []\n",
    "results = binaryclass_predictor.predict(test_X)\n",
    "predictions += [r['predicted_label'] for r in results['predictions']]\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_context(\"paper\", font_scale=1.4)\n",
    "\n",
    "y_test = (data_test['MEDV']> 0.377777)+0\n",
    "y_pred = predictions\n",
    "\n",
    "class_names = np.array(['YES', 'NO'])\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix',\n",
    "                      cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test = (data_test['MEDV']> 0.377777)+0\n",
    "y_pred = predictions\n",
    "scores = precision_recall_fscore_support(y_test, y_pred, average='macro',labels=np.unique(y_pred))\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:{}'.format(acc))\n",
    "print('Precision:{}'.format(scores[0]))\n",
    "print('Recall :{}'.format(scores[1]))\n",
    "print('F1 score:{}'.format(scores[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
